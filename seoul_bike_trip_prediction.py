# -*- coding: utf-8 -*-
"""seoul_bike_trip_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OnfchNBvvxOzGDUUdKy2jckz9g_LuYyn

<center><h1>SEOUL BIKE TRIP DURATION PREDICTION</h1></center>
<center><img src="https://storage.googleapis.com/kaggle-datasets-images/1182717/1978889/22959fc75a57e2249067c80aba6b338c/dataset-cover.jpg?t=2021-02-26-19-04-48" /><center>

<b>Problem Statement: </b>The duration of a trip is the most crucial measure in all modes of transportation. Hence, it is crucial to predict the trip time precisely for the advancement of Intelligent Transport Systems (ITS) and traveler information systems. The prediction can be done using the Seoul Bike data and weather data. We need to use this combination of Seoul Bike data and weather data to do the trip duration prediction.​

## ABOUT DATASET 
- <b>Dataset File Name:</b> `for_modeling.csv`
- <b>Dataset Shape:</b> (9601139, 26)
- <b>Columns in the dataset:</b>
    - Unnamed: 0
    - Duration 
    - Distance
    - PLong
    - PLatd
    - DLong
    - DLatd
    - Haversine
    - Pmonth
    - Pday
    - Phour
    - Pmin
    - PDweek
    - Dmonth
    - Dday
    - Dhour
    - Dmin
    - DDweek
    - Temp
    - Precip
    - Wind
    - Humid
    - Solar
    - Snow
    - GroundTemp
    - Dust

### Importing required libraries
"""

# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
import random
import warnings

np.random.seed(24)
random.seed(24)
warnings.filterwarnings("ignore")

# import dataset
from google.colab import drive
drive.mount('/content/drive')

"""### Importing the dataset"""

# Read csv using pandas
train = pd.read_csv('/content/drive/MyDrive/seoul bike trip/For_modeling.csv.zip')

# check the shape of the data
train.shape

# sample 100000 rows of data
train = train.sample(n=100000).reset_index().drop(columns=['index'])

# check the shape
train.shape

# check the top 5 rows
train.head()

"""## Exploring the Data
### Identifying the number of features or columns
"""

# Check all the columns in the dataset
train.columns

"""### Drop irrelevant columns from dataset"""

# Dropping irrelevant column from the dataset i.e Unnamed: 0
train.drop('Unnamed: 0', axis=1, inplace=True)

"""### Knows more about the data in the columns like data type it contains and total samples of each"""

# Check which columns are having categorical, numerical or boolean values
train.info()

"""**After checking the Dtypes of all the columns**

**object - String values**

**float64 - Numerical values**

**Observation: There are no String values so there are no categorical data**

### Know more mathematical relations of the dataset like count, min, max values, standarad deviation values, mean and different percentile values
"""

# For more information on the dataset like the total count in all the columns
# min, max values and more information of the respective columns 
train.describe()

"""### Get the total number of samples in the dataset using the len() function"""

# check length of dataset
len(train)

"""### Get unique values"""

# check for unique values in dataset
train.nunique()

"""### Counting the total number of missing value"""

# Check for missing values in all the columnns of the dataset
train.isnull().sum()

"""There is no missing values in this dataset

## Exploratory Data Analysis (EDA)

### CORRELATION MATRIX


Why ?

**A correlation matrix is a table showing correlation coefficients between variables**

There are three broad reasons for computing a correlation matrix:
1. To summarize a large amount of data where the goal is to see patterns. In our example above, the observable pattern is that all the variables highly correlate with each other.
2. To input into other analyses. For example, people commonly use correlation matrixes as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.
3. As a diagnostic when checking other analyses. For example, with linear regression, a high amount of correlations suggests that the linear regression estimates will be unreliable.
"""

#correlation matrix Using pandas
corr = train.corr()
corr.style.background_gradient(cmap='coolwarm').set_precision(2)

# correlation matrix Using seaborn
plt.figure(figsize=(20,12))
ax = sns.heatmap(corr,vmin=-1, vmax=1, center=0,
                 cmap=sns.diverging_palette(20,200, n=200),
                 square=True)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45, horizontalalignment='right');

"""**Observation from above correlation matrix**
1. distance and duration are strongly related
2. haversine and duration and correlated with each other
3. gound temperature and solar radiation are correlated
4. temperature and solar radiations are correlated
5. phour and dhour are not related to humidity

## SCATTER PLOT

1. A scatter plot is a type of plot using Cartesian coordinates to display values for typically two variables for a set of data.

2. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.

3. Scatter plot's are used to observe and show relationships between two numeric variables.
"""

# Scatter plot using matplotlib 
# create function for ploting scatterplot between two columns of dataset
def plot_scatter(x,y):
  plt.figure()
  plt.xlabel(x)
  plt.ylabel(y)
  plt.scatter(train[x],train[y])
  plt.show()

# Loop through numerical data list and use function to scatter plot between two columns
column_list = train.columns
for i in column_list:
  for j in column_list:
     if i != j:
       plot_scatter(i,j)

"""PLong and DLong are correlated

PLatd and DLatd are correlated

Pmonth and Dmonth are correlated

## HISTOGRAM

1. A histogram is an approximate representation of the distribution of numerical data.

2. To construct a histogram, the first step is to "bin" (or "bucket") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.

3. The words used to describe the patterns in a histogram are: "symmetric", "skewed left" or "right", "unimodal", "bimodal" or "multimodal".
"""

#Histogram Using pandas 
train.hist(figsize=(18,15), legend=True);

"""Observation -
2. data distribution of duration, distance, haversine, precipation, solar radiation, snow and dust are skewed right.
3. data distribution of humidity is symmetrical
4. data distribution of Pmin and Dmin is right skewed.
4. data distribution of Phour and Dhour is same.
4. data distribution of pday and Dday is same.
4. data distribution of Pmonth and Dmonth is same

## VIF - Variance inflation factor
1. The variance inflation factor (VIF) quantifies the extent of correlation between one predictor and the other predictors in a model.
2. It is used for diagnosing collinearity/multicollinearity.
3. Higher values signify that it is difficult to impossible to assess accurately the contribution of predictors to a model.
"""

# import statsmodle library for vif 
from statsmodels.stats.outliers_influence import variance_inflation_factor

# creating a dataframe of just numerical values
train_vif = train.drop(['Duration'], axis=1)
# target values
y = train['Duration']
# numerical values column names

print(train_vif.columns)

names = train_vif.columns
names

# droping rows with empty cells
train_vif.columns == names[1]

train_vif

# Calculating VIF for each feature.
import statsmodels.api as sm

for i in range (0, len(names)):
  y = train_vif.loc[:, train_vif.columns == names[i]]
  x = train_vif.loc[:, train_vif.columns != names[i]]

  model = sm.OLS(y,x)
  results = model.fit()
  rsq = results.rsquared
  VIF = round(1/(1-rsq),2)

  print("R Square value of {} column is {} keeping all other columns as features".format(names[i],round(rsq,2)))
  print("Variance inflation Factor of {} column is {} \n".format(names[i], VIF))

"""there is colinearity/multicolinearity between variables as the VIF value is almost upto 2.5

Distance', 'PLong', 'PLatd', 'DLong','DLatd',  'Phour',  'PDweek', 'Dday', 'Dhour', 'Dmin', 'DDweek', 'Temp', 'Precip', 'Humid', 'Solar', 'Snow', 'GroundTemp',  have colinearity with all the variables.

## Box Plot

A boxplot is a standardized way of displaying the dataset based on a five-number summary:

    1. Minimum (Q0 or 0th percentile): the lowest data point excluding any outliers.

    2. Maximum (Q4 or 100th percentile): the largest data point excluding any outliers.

    3. Median (Q2 or 50th percentile): the middle value of the dataset.

    4. First quartile (Q1 or 25th percentile): also known as the lower quartile qn(0.25), is the median of the lower half of the dataset.

    5. Third quartile (Q3 or 75th percentile): also known as the upper quartile qn(0.75), is the median of the upper half of the dataset
"""

# Perform boxplot on Duration column of the dataset
train.boxplot(column = 'Duration', figsize=(12,10))

"""from above box plot graph:

duration

25% of duration have value between range 0 to 8

25% of duration have value between range 8 to 18

25% of duration have value between range 18 to 35

25% of duration have value between range 35 to 77

The mean duration is around 18

**Mostly the duration are on the lower side of the spectrum. Means small duration trips are the common trips**

"""

# Perform boxplot on Distance column of the dataset
train.boxplot(column='Distance', figsize=(12,10))

"""from above box plot graph:

distance

25% of distance have value between range 0 to 1000

25% of distance have value between range 1000 to 2500

25% of distance have value between range 2500 to 4900

25% of distance have value between range 4900 to 9900

The mean distance is around 2500

 Mostly the distance are on the lower side of the spectrum. Means small distance trips are the common trips


"""

# Perform boxplot on PLong column of the dataset
train.boxplot(column='PLong', figsize=(12,10))

"""from above box plot graph:

PLong

25% of PLong have value between range 37.44 to 37.52

25% of PLong have value between range 37.52 to 37.559

25% of PLong have value between range 37.559 to 37.58

25% of PLong have value between range 37.58 to 37.66

The mean PLong is around 37.559
"""

# Perform boxplot on PLatd column of the dataset
train.boxplot(column='PLatd', figsize=(12,10))

"""from above box plot graph:

PLatd

25% of PLatd have value between range 126.80 to 162.92

25% of PLatd have value between range 126.92 to 126.99

25% of PLatd have value between range 126.99 to 127.07

25% of PLatd have value between range 127.07 to 127.18

The mean PLatd is around 126.99
"""

# Perform boxplot on Dlong column of the dataset
train.boxplot(column='DLong', figsize=(12,10))

"""from above box plot graph:

DLong

25% of DLong have value between range 37.43 to 37.52

25% of DLong have value between range 37.52 to 37.545

25% of DLong have value between range 37.545 to 37.575

25% of DLong have value between range 37.575 to 37.665

The mean DLong is around 37.545
"""

# Perform boxplot on DLatd column of the dataset
train.boxplot(column='DLatd', figsize=(12,10))

"""from above box plot graph:

DLatd

25% of DLatd have value between range 126.80 to 126.92

25% of DLatd have value between range 126.92 to 126.995

25% of DLatd have value between range 126.995 to 127.055

25% of DLatd have value between range 127.055 to 127.175

The mean DLatd is around 126.995
"""

# Perform boxplot on Haversine column of the dataset
train.boxplot(column='Haversine', figsize=(12,10))

"""from above box plot graph:

haversine

25% of haversine have value between range 0 to 0.25

25% of haversine have value between range 0.25 to 1

25% of haversine have value between range 1 to 2.5

25% of haversine have value between range 2.5 to 5

The mean haversine is around 1

Observation: Mostly the haversine are on the lower side of the spectrum. Means small distance trips are the common trips
"""

# Perform boxplot on pmonth column of the dataset
train.boxplot(column='Pmonth', figsize=(12,10))

"""from above box plot graph:

Pmonth

25% of Pmonth have value between range 1 to 6

25% of Pmonth have value between range 6 to 8

25% of Pmonth have value between range 8 to 10

25% of Pmonth have value between range 10 to 12

The mean Pmonth is around 8

more pickups are in the month which are at the end of the year
"""

# Perform boxplot on Pday column of the dataset
train.boxplot(column='Pday', figsize=(12,10))

"""from above box plot graph:

Pday

25% of Pday have value between range 1 to 8

25% of Pday have value between range 8 to 16

25% of Pday have value between range 16 to 23

25% of Pday have value between range 23 to 31

The mean Pday is around 16

pickups are almost same on every day of month
"""

# Perform boxplot on Phour column of the dataset
train.boxplot(column='Phour', figsize=(12,10))

"""from above box plot graph:

Phour

25% of Phour have value between range 0 to 10

25% of Phour have value between range 10 to 16

25% of Phour have value between range 16 to 19

25% of Phour have value between range 19 to 24

The mean Phour is around 16

more pickups are towards the end of the day
"""

# Perform boxplot on Pmin column of the dataset
train.boxplot(column='Pmin', figsize=(12,10))

"""from above box plot graph:

Pmin

25% of Pmin have value between range 0 to 14

25% of Pmin have value between range 14 to 29

25% of Pmin have value between range 29 to 44

25% of Pmin have value between range 44 to 60

The mean Pmin is around 29
"""

# Perform boxplot on PDweek column of the dataset 
train.boxplot(column='PDweek', figsize=(12,10))

"""from above box plot graph:

PDweek

25% of PDweek have value between range 0 to 1

25% of PDweek have value between range 1 to 3

25% of PDweek have value between range 3 to 5

25% of PDweek have value between range 3 to 6

The mean PDweek is around 3

equal pickups throughout the week
"""

# Perform boxplot on Dmonth column of the dataset
train.boxplot(column='Dmonth', figsize=(12,10))

"""from above box plot graph:

Dmonth

25% of Dmonth have value between range 1 to 6

25% of Dmonth have value between range 6 to 8

25% of Dmonth have value between range 8 to 10

25% of Dmonth have value between range 10 to 12

The mean Dmonth is around 8

more drop offs are towords the end months of the year
"""

# Perform boxplot on Dday column of the dataset 
train.boxplot(column='Dday', figsize=(12,10))

"""from above box plot graph:

Dday

25% of Dday have value between range 1 to 8

25% of Dday have value between range 8 to 16

25% of Dday have value between range 16 to 23

25% of Dday have value between range 23 to 31

The mean Dday is around 16

drops the equal throughout all the days of month
"""

# Perform boxplot on Dhour column of the dataset
train.boxplot(column='Dhour', figsize=(12,10))

"""from above box plot graph:

Dhour

25% of Dhour have value between range 0 to 10

25% of Dhour have value between range 10 to 16

25% of Dhour have value between range 16 to 20

25% of Dhour have value between range 20 to 24

The mean Dhour is around 16

drops offs are more towards the end hours of the day
"""

# Perform boxplot on Dmin column of the dataset
train.boxplot(column='Dmin', figsize=(12,10))

"""from above box plot graph:

Dmin

25% of Dmin have value between range 0 to 15

25% of Dmin have value between range 15 to 30

25% of Dmin have value between range 30 to 45

25% of Dmin have value between range 45 to 60

The mean Dmin is around 30
"""

# Perform boxplot on DDweek column of the dataset
train.boxplot(column='DDweek', figsize=(12,10))

"""from above box plot graph:

DDweek

25% of DDweek have value between range 0 to 1

25% of DDweek have value between range 1 to 3

25% of DDweek have value between range 3 to 5

25% of DDweek have value between range 5 to 6

The mean DDweek is around 3

drops are equal throughout the week
"""

# Perform boxplot on Temp column of the dataset
train.boxplot(column='Temp', figsize=(12,10))

"""from above box plot graph:

temperature

25% of temperature have value between range -8 to 2

25% of temperature have value between range 2 to 20

25% of temperature have value between range 20 to 26

25% of temperature have value between range 26 to 40

The mean temperature is around 20

trips are planned in slightly high temperature
"""

# Perform boxplot on Precip column of the dataset
# Using pandas 
train.boxplot(column='Precip', figsize=(12,10))

"""from above box plot graph:

precip

25% of precip have value between range 0 to 0

25% of precip have value between range 0 to 0

25% of precip have value between range 0 to 0

25% of precip have value between range 0 to 0

The mean precip is around 0

more trips are planned when there is no rain or snow fall
"""

# Perform boxplot on wind column of the dataset
train.boxplot(column='Wind', figsize=(12,10))

"""from above box plot graph:

wind

25% of wind have value between range 0 to 1.1

25% of wind have value between range 1.1 to 1.8

25% of wind have value between range 1.8 to 2.5

25% of wind have value between range 2.5 to 4.2

The mean wind is around 1.8

more trips are during less windy wheather
"""

# Perform boxplot on humid column of the dataset
train.boxplot(column='Humid', figsize=(12,10))

"""from above box plot graph:

humid

25% of humid have value between range 10 to 41

25% of humid have value between range 41 to 56

25% of humid have value between range 56 to 64

25% of humid have value between range 64 to 99

The mean humid is around 56
"""

# Perform boxplot on solar column of the dataset 
train.boxplot(column='Solar', figsize=(12,10))

"""from above box plot graph:

solar

25% of solar have value between range 0 to 0

25% of solar have value between range 0 to 0.25

25% of solar have value between range 0.25 to 1.25

25% of solar have value between range 1.25 to 3.1

The mean solar is around 0.25
"""

# Perform boxplot on snow column of the dataset 
train.boxplot(column='Snow', figsize=(12,10))

"""from above box plot graph:

snow

25% of snow have value between range 0 to 0

25% of snow have value between range 0 to 0

25% of snow have value between range 0 to 0

25% of snow have value between range 0 to 0

The mean snow is around 0

more trips are during when there is no snow fall
"""

# Perform boxplot on GroundTemp column of the dataset
train.boxplot(column='GroundTemp', figsize=(12,10))

"""from above box plot graph:

groundtemperature

25% of groundtemperature have value between range -12 to 11

25% of groundtemperature have value between range 11 to 21

25% of groundtemperature have value between range 21 to 29

25% of groundtemperature have value between range 29 to 52

The mean groundtemperature is around 21
"""

# Perform boxplot on dust column of the dataset
train.boxplot(column='Dust', figsize=(12,10))

"""from above box plot graph:

dust

25% of dust have value between range 0 to 20

25% of dust have value between range 20 to 30

25% of dust have value between range 30 to 45

25% of dust have value between range 45 to 80

The mean dust is around 30

Observation: Mostly the dust are on the lower side of the spectrum. Means people preffer to travel in less dust concentration

## POINT PLOT

1. A point plot uses scatter plot glyphs to visualize features like point estimates and confidence intervals.

2. A point plot uses scatter plot points to represent the central tendency of numeric data.

3. These plots make use of error bars to indicate any uncertainty around the numeric
"""

# Perform point plot between Phour and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Phour', y='Duration', data=train, palette='rainbow')

"""duration is more during morning and evening

after 8 duration is increasing and after 20 its again decreasing
"""

# Perform point plot between distance and Duration values 
plt.figure(figsize=(25,6))
sns.pointplot(x='Duration', y='Distance', data=train, palette='rainbow')

"""duration and distance have a positive correlation between them



as duration increases distance also increases
"""

# Perform point plot between PLong and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='PLong', data=train, palette='rainbow')

"""as duration  increases  pickup longitude decreases"""

# Perform point plot between PLatd and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='PLatd', data=train, palette='rainbow')

"""duration and PLatd has negative correlation between them.

as the value of duration increases value of PLatd decreases
"""

# Perform point plot between DLong and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='DLong', data=train, palette='rainbow')

"""duration and DLong has negative correlation between them.

as the value of duration increases value of DLong decreases
"""

# Perform point plot between haversine and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Haversine', data=train, palette='rainbow')

"""duration and haversine are correlated 

they have positive correlation between them
"""

# Perform point plot between Pmonth and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Pmonth', data=train, palette='rainbow')

"""pmonth and duration are positively correlated

as the value of duration increases value of Pmonth also increases





"""

# Perform point plot between Pday and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Pday', data=train, palette='rainbow')

"""most of the points are between 14.0 and 15.0

very few are above 15.0 and below 14.0
"""

# Perform point plot between PDweek and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='PDweek', data=train, palette='rainbow')

"""Duration has a positive correlation with PDweek

as the value of duration increasing value of PDweek is also increasing
"""

# Perform point plot between Dmonth and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Dmonth', data=train, palette='rainbow')

"""all values are between 7.5 and 7.8 

there is no relation between Dmonth and Duration
"""

# Perform point plot between Dhour and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Dhour', data=train, palette='rainbow')

"""there is a positive relation between duration and Dhour 

as the value of duration increases value of Dhour also increases
"""

# Perform point plot between DDweek and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='DDweek', data=train, palette='rainbow')

"""there is increase in DDweek when there is increase in duration

they are correlated to each other
"""

# Perform point plot between Temp and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Temp', data=train, palette='rainbow')

"""duration and templerature have a positive relation between them

"""

# Perform point plot between Precip and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Precip', data=train, palette='rainbow')

"""most of the values are between 0.0 and 0.01

very few values are above 0.01
"""

# Perform point plot between wind and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Wind', data=train, palette='rainbow')

"""duration and wind are correlated 

as the value of duration increases value of wind is also increasing
"""

# Perform point plot between humid and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Humid', data=train, palette='rainbow')

"""duration and humidity are negatively correlated 

as the value of duration is increasing value of humidity decreases
"""

# Perform point plot between solar and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Solar', data=train, palette='rainbow')

"""as the value of duration is increasing value of solar radiations is also increasing

they are correlated to each other
"""

# Perform point plot between snow and Duration values 
plt.figure(figsize=(25,6))
sns.pointplot(x='Duration', y='Snow', data=train, palette='rainbow')

"""snow and duration are correlated 

they have negative relation between them
"""

# Perform point plot between GroundTemp and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='GroundTemp', data=train, palette='rainbow')

"""ground temperature and duration are correlated to each other.

as the value of duration increases value of ground temperature also increases
"""

# Perform point plot between dust and Duration values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Dust', data=train, palette='rainbow')

"""dust and duration are correlated to each other.

as the value of duration increases value of dust also increases
"""

# Perform point plot between precip and distance values 
plt.figure(figsize=(20,6))
sns.pointplot(x='Duration', y='Precip', data=train, palette='rainbow')

"""all the values are around 0.00

very few are above 0.00
"""

# Perform point plot between PLong and Dlong values 
plt.figure(figsize=(20,6))
sns.pointplot(x='PLong', y='DLong', data=train, palette='rainbow')

"""Plong and Dlong are corelated"""

# Perform point plot between PLong and haversine values 
plt.figure(figsize=(25,6))
sns.pointplot(x='PLong', y='Haversine', data=train, palette='rainbow')

"""all the values are between 1 and 3

very few are above 3 and below 1
"""

# Perform point plot between PLong and Pday values 
plt.figure(figsize=(25,6))
sns.pointplot(x='PLong', y='Pday', data=train, palette='rainbow')

"""all the values are between 12.5 to 17.5

very few are above 17.5 and below 12.5
"""

# Perform point plot between PLong and Pmin values 
plt.figure(figsize=(20,6))
sns.pointplot(x='PLong', y='Pmin', data=train, palette='rainbow')

"""all the values are between 25 to 35

very few are above and below of this range
"""

# Perform point plot between PLong and PDweek values 
plt.figure(figsize=(20,6))
sns.pointplot(x='PLong', y='PDweek', data=train, palette='rainbow')

"""all the values are between 2 to 4

very few are above and below of this range
"""

# Perform point plot between PLong and Dday values 
plt.figure(figsize=(25,6))
sns.pointplot(x='PLong', y='Dday', data=train, palette='rainbow')

"""all the values are between 12 to 17

very few are above and below of this range
"""

# Perform point plot between PLong and Precip values 
plt.figure(figsize=(25,6))
sns.pointplot(x='PLong', y='Precip', data=train, palette='rainbow')

"""all values are around 0.00 

very few are above 0.00
"""

# Perform point plot between PLong and Humind values 
plt.figure(figsize=(20,6))
sns.pointplot(x='PLong', y='Humid', data=train, palette='rainbow')

"""as the distribution value of PLong is increasing the value of distribution of humidity is same"""

# Perform point plot between PLatd and Dday values 
plt.figure(figsize=(25,6))
sns.pointplot(x='PLatd', y='Dday', data=train, palette='rainbow')

"""all valies are around 15

very few are above and below of 15

## COUNT PLOT

1. A countplot is kind of like a histogram or a bar graph for some categorical area.

2. It simply shows the number of occurrences of an item based on a certain type of category.
"""

# Perform the countplot on the pickup hour data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Phour']).set_title('PICKUP HOURS DISTRIBUTION')
plt.show()

"""more pickups are on morning and evening """

# Perform the countplot on the drop hour data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Dhour']).set_title('DROP HOURS DISTRIBUTION')
plt.show()

"""Observation:  Dropoff hour are high at the office hours both at morning and evening


"""

# Perform the countplot on the duration data
plt.figure(figsize=(25,9))
sns.countplot(x=train['Duration']).set_title('Duration DISTRIBUTION')
plt.show();

"""distribution of duration is skewed left side

short duration trips are common
"""

# Perform the countplot on the distance data
plt.figure(figsize=(25,9))
sns.countplot(x=train['Distance']).set_title('Distance DISTRIBUTION')
plt.show();

"""distribution of data is skewed towards left

short distance trips are common
"""

# Perform the countplot on the PLong data
plt.figure(figsize=(25,9))
sns.countplot(x=train['PLong']).set_title('PLong DISTRIBUTION')
plt.show();

"""distribution of data is multimodal"""

# Perform the countplot on the Platd data

plt.figure(figsize=(18,9))
sns.countplot(x=train['PLatd']).set_title('PLatd DISTRIBUTION')
plt.show();

"""distribution of data is multimodal"""

# Perform the countplot on the Dlong data
plt.figure(figsize=(25,9))
sns.countplot(x=train['DLong']).set_title('DLong DISTRIBUTION')
plt.show();

"""distribution of data is multimodal"""

# Perform the countplot on the Dlatd data
plt.figure(figsize=(25,9))
sns.countplot(x=train['DLatd']).set_title('DLatd DISTRIBUTION')
plt.show();

"""distribution of data is multimodal"""

# Perform the countplot on the Pmonth data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Pmonth']).set_title('Pmonth DISTRIBUTION')
plt.show();

"""more picks are on end of the year"""

# Perform the countplot on the Pday data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Pday']).set_title('Pday DISTRIBUTION')
plt.show()

"""distribution of data is multimodal

pickups are  almost similar on every day of a month
"""

# Perform the countplot on the pmin data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Pmin']).set_title('Pmin DISTRIBUTION')
plt.show();

"""data is distributed equally"""

# Perform the countplot on the PDweek data
plt.figure(figsize=(18,9))
sns.countplot(x=train['PDweek']).set_title('PDweek DISTRIBUTION')
plt.show();

"""data is distributed equally"""

# Perform the countplot on the Dday data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Dday']).set_title('Dday DISTRIBUTION')
plt.show();

"""distribution of data is similar

drop-offs are almost similar on every day of a month
"""

# Perform the countplot on the Dmin data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Dmin']).set_title('Dmin DISTRIBUTION')
plt.show();

"""data is distribited equally

drops on every minute of hour is similar
"""

# Perform the countplot on the DDweek data
plt.figure(figsize=(18,9))
sns.countplot(x=train['DDweek']).set_title('DDweek DISTRIBUTION')
plt.show();

"""drop offs on every day of a week are almost similar"""

# Perform the countplot on the Temp data
plt.figure(figsize=(25,9))
sns.countplot(x=train['Temp']).set_title('Temp DISTRIBUTION')
plt.show();

"""the distribution of data is skewed toward right

trips are avoided in less temperature
"""

# Perform the countplot on the Precip data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Precip']).set_title('Precip DISTRIBUTION')
plt.show();

"""the distribution of data is skewed towards left

no trips are planned during rainy or snowy wheather
"""

# Perform the countplot on the wind data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Wind']).set_title('Wind DISTRIBUTION')
plt.show();

"""the distribution of data is skewed towards left

trips are mostly avoided in windy wheather
"""

# Perform the countplot on the himid hour data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Humid']).set_title('Humid DISTRIBUTION')
plt.show();

"""people mostly avoid to go on trips on less humid or in extra humid wheather"""

# Perform the countplot on the solar data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Solar']).set_title('Solar DISTRIBUTION')
plt.show();

"""the data distribution is skewed towards left side"""

# Perform the countplot on the snow data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Snow']).set_title('Snow DISTRIBUTION')
plt.show();

"""the data distribution is skewed towards left side

more trips are during  when there is no snowfall.
"""

# Perform the countplot on the GroundTemp data
plt.figure(figsize=(18,9))
sns.countplot(x=train['GroundTemp']).set_title('GroundTemp DISTRIBUTION')
plt.show();

"""the distribution of data is multimodel"""

# Perform the countplot on the dust data
plt.figure(figsize=(18,9))
sns.countplot(x=train['Dust']).set_title('Dust DISTRIBUTION')
plt.show();

"""distribution of data is skewed towards left

more trips are done when there is less dust

## Boxen Plot

The boxen plot, otherwise known as a Letter-value plot, is a box plot meant for large data sets (n > 10,000).

The Boxen plot is very similar to box plot, except for the fact that it plots different quartile values.

By plotting different quartile values, we are able to understand the shape of the distribution particularly in the head end and tail end.
"""

# Perform boxen plot between distance and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Distance', data=train, palette='rainbow');

"""distribution of distance is increasing as the duration  is increasing"""

# Perform boxen plot between Duration and Plong 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='PLong', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.
There is no relation between Plong and  Duration
"""

# Perform boxen plot between PLatd and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='PLatd', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.
There is no relation between duration and Platd


"""

# Perform boxen plot between DLong and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='DLong', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.
There is no relation between duration and Dlong
"""

# Perform boxen plot between Dlatd and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='DLatd', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and Dlatd
"""

# Perform boxen plot between Pday and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Pday', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and pday
"""

# Perform boxen plot between Phour and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Phour', data=train, palette='rainbow');

"""there is weak relation between duration and Phours"""

# Perform boxen plot between Pmin and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Pmin', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and Pmin
"""

# Perform boxen plot between Dday and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dday', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and Dday
"""

# Perform boxen plot between Dhour and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dhour', data=train, palette='rainbow');

"""there is weak relation between duration and Dhour"""

# Perform boxen plot between Dmin and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dmin', data=train, palette='rainbow');

"""there is no relation between duration and Dmin"""

# Perform boxen plot between Dday and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dday', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and Dday
"""

# Perform boxen plot between Dmin and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dmin', data=train, palette='rainbow');

"""there is no relation between duration and Dmin"""

# Perform boxen plot between temp and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Temp', data=train, palette='rainbow');

"""there is positive relation between duration and temperature"""

# Perform boxen plot between precip and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Precip', data=train, palette='rainbow');

"""no relation between duration and precipitation"""

# Perform boxen plot between wind and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Wind', data=train, palette='rainbow');

"""from above boxen plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. There is no relation between duration and wind
"""

# Perform boxen plot between humid and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Humid', data=train, palette='rainbow');

"""duration and humidity have weak correlation between them"""

# Perform boxen plot between solar and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Solar', data=train, palette='rainbow');

"""solar raditation and duration are correlated to eachother.

as the solar radiation increases duration also increases
"""

# Perform boxen plot between snow and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Snow', data=train, palette='rainbow');

"""no relation between snow and duration"""

# Perform boxen plot between GroundTemp and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='GroundTemp', data=train, palette='rainbow');

"""duration and ground temperature are correlated"""

# Perform boxen plot between dust and Duration 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Duration', y='Dust', data=train, palette='rainbow');

"""there is not much difference in the value of dust as duration increases.

there is weak relation between duration and dust.
"""

# Perform boxen plot between distance and haversine 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Haversine', data=train, palette='rainbow');

"""there is weak relation between distance and haversine"""

# Perform boxen plot between distance and Pmonth 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Pmonth', data=train);

"""there is no relation between distance and Pmonth"""

# Perform boxen plot between distance and Pday 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Pday', data=train, palette='rainbow');

"""no relation between distance Pday"""

# Perform boxen plot between distance and Pmin 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Pmin', data=train, palette='rainbow');

"""there is no relation between distance and Pmin"""

# Perform boxen plot between distance and PDweek 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='PDweek', data=train, palette='rainbow');

"""there is no relation between distance and PDweek"""

# Perform boxen plot between distance and Dmonth 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Dmonth', data=train, palette='rainbow');

"""there is no relation bertween distance and Dmonth"""

# Perform boxen plot between distance and Dday 
plt.figure(figsize=(25,10))
sns.boxenplot(x='Distance', y='Dday', data=train, palette='rainbow');

"""there is no relation between distance and Dday

## DENDOGRAM

The dendrogram is a visual representation of the compound correlation data. The individual compounds are arranged along the bottom of the dendrogram and referred to as leaf nodes. Compound clusters are formed by joining individual compounds or existing compound clusters with the join point referred to as a node.
"""

from seaborn.matrix import dendrogram
# Plot a Dendrogram on the columns of the dataset
import scipy
from scipy.cluster import hierarchy as hc

corr = np.round(scipy.stats.spearmanr(train).correlation, 4)
corr_condensed = hc.distance.squareform(1 - corr)
z = hc.linkage(corr_condensed, method='average')
fig= plt.figure(figsize=(18,9))
dendrogram = hc.dendrogram(z,labels=train.columns, orientation='left', leaf_font_size=16)
plt.show()

"""Observation: All connected are compoundly related to each other like latitudes to longitudes, distance to duration and many more.

## Violin Plot

1. A violin plot is a method of plotting numeric data.

1. Violin plots are similar to box plots, except that they also show the probability density of the data at different values, usually smoothed by a kernel density estimator.

3. It has:

    1. Median (a white dot on the violin plot)
    2. Interquartile range (the black bar in the center of violin)
    3. The lower/upper adjacent values (the black lines stretched from the bar) — defined as first quartile — 1.5 IQR and third quartile + 1.5 IQR respectively.
"""

# Perform violin plot between distance and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Distance', data=train);

"""Distance and duration arecorrelated

as duration increases distance also increases
"""

# Perform violin plot between PLong and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='PLong', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.
There is no relation between duration and PLong 
"""

# Perform violin plot between Platd and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='PLatd', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.

There is no relation between duration and PLatd 
"""

# Perform violin plot between Dlong and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='DLong', data=train);

"""Dlong distribution for all values of duration is simmilar.

there is no relation between DLong and duration.
"""

# Perform violin plot between DLatd and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='DLatd', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.
There is no relation between duration and DLatd 
"""

# Perform violin plot between Pmin and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Pmin', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. 

There is no relation between duration and Pmin
"""

# Perform violin plot between Dhour and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Dhour', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical. 

There is no relation between duration and Dday
"""

# Perform violin plot between Dmin and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Dmin', data=train);

"""Dmin distribution for all values of Duration is similar."""

# Perform violin plot between Precip and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Precip', data=train);

"""precip distribution for all values of duration is simmilar."""

# Perform violin plot between wind and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Wind', data=train);

"""wind distribution for all values of duration is simmilar.

"""

# Perform violin plot between humid and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Humid', data=train);

"""as duration increases there is slight change in the humidity 

humidity is decreasing as duration is increasing
"""

# Perform violin plot between solar and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Solar', data=train);

"""solar radiation and duration have positive correlation

as value of solar radiation increases duration also increases.
"""

# Perform violin plot between GroundTemp and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='GroundTemp', data=train);

"""as the value of duration increases value of ground temperature also slightly increases 

there is weak correlation between ground temperature and duration.
"""

# Perform violin plot between dust and Duration 
plt.figure(figsize=(28,10))
sns.violinplot(x='Duration', y='Dust', data=train);

"""from above violin plot:

The distribution between lower adjacent value and upper adjacent value is symmetrical.

There is weak relation between duration and dust
"""

# Perform violin plot between distance and Plong 
plt.figure(figsize=(28,10))
sns.violinplot(x='Distance', y='PLong', data=train);

"""there is no relation between PLong and Distance"""

# Perform violin plot between distance and PLatd 
plt.figure(figsize=(28,10))
sns.violinplot(x='Distance', y='PLatd', data=train);

"""there is no relation between PLatd and Distance"""

# Perform violin plot between distance and Dlong 
plt.figure(figsize=(28,10))
sns.violinplot(x='Distance', y='DLong', data=train);

"""there is no relation between DLatd and Distance"""

# Perform violin plot between distance and Dlatd 
plt.figure(figsize=(28,10))
sns.violinplot(x='Distance', y='DLatd', data=train);

"""there is no relation between DLatd and Distance"""

# Perform violin plot between distance and Pday
plt.figure(figsize=(28,10))
sns.violinplot(x='Distance', y='Pday', data=train);

"""there is no relation between Pday and Distance

## Modelling

NOTE:
The dataset has 9.6 million samples.

We have used only 100000 samples for training.

If you want you can use complete dataset.

Using complete dataset will take longer time to train the model.
"""



# Splitting data into Labels and target
X = train.drop('Duration', axis=1)
y = train['Duration']

X.head()

y

# Splitting the dataset into train and test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)

# Check the shape of all the splitted dataset
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# importing necessary libraries for geting metrics of models
import math
import sklearn.metrics as metrics
from sklearn.metrics import median_absolute_error

# Function for calculating RMSE 
def rmse(x,y): 
    return math.sqrt(((x-y)**2).mean())

# Function for calculating all the relevant metrics 
def print_score(m):
    res = [rmse(m.predict(X_train), y_train),rmse(m.predict(X_test), y_test),
           m.score(X_train, y_train),m.score(X_test, y_test),
           median_absolute_error(m.predict(X_train), y_train),median_absolute_error(m.predict(X_test), y_test),
           metrics.mean_absolute_error(m.predict(X_train), y_train),metrics.mean_absolute_error(m.predict(X_test), y_test),
          
          ]
    

    print("RMSE-Train: " + str(res[0]) + "\nRMSE-Test: " + str(res[1]) + "\nScore-Train: " + str(res[2]) + "\nScore-Test: " + str(res[3]) +
         "\nMedAE-Train: " + str(res[4]) + "\nMedAE-Test: " + str(res[5]) + "\nMeanAE-Train: " + str(res[6]) + "\nMeanAE-Test: " + str(res[7]))

# Visualize importance of all the features in the dataset for the prediction

def visualize_importance(feature_importances, feat_train_df):
    feature_importance_df = pd.DataFrame()
     # creating dataframe for feature name and feature importance
    _df = pd.DataFrame()
    _df['feature_importance'] = feature_importances
    _df['column'] = feat_train_df.columns
    feature_importance_df = pd.concat([feature_importance_df, _df], 
                                      axis=0, ignore_index=True)
    # grouping all data and sorting in descending order
    order = feature_importance_df.groupby('column')\
        .sum()[['feature_importance']]\
        .sort_values('feature_importance', ascending=False).index[:50]
    # ploting feature importance data using boxenplot
    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))
    sns.boxenplot(data=feature_importance_df, 
                  x='feature_importance', 
                  y='column', 
                  order=order, 
                  ax=ax, 
                  palette='viridis', 
                  orient='h')
    ax.tick_params(axis='x', rotation=0)
    ax.set_title('Importance')
    ax.grid()
    fig.tight_layout()
    # return fig, ax
    return fig, ax

"""# Linear Regression

Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Fit a Linear Regression model to the train dataset
# 
# # Import LinearRegressor
# from sklearn.linear_model import LinearRegression
# 
# # Instantiate the model
# LR = LinearRegression()
# 
# # Fit the model to the data
# LR.fit(X_train, y_train)
# 
# # print score of the model
# print_score(LR)
# 
# # visualizing the inportance of features.
# visualize_importance(LR.coef_, X_train)

"""## Random Forest Regressor

Random forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. It is also one of the most used algorithms, because of its simplicity and diversity.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Fit a Random Forest Regressor model to the train dataset
# 
# # Import RandomForrestRegressor
# from sklearn.ensemble import RandomForestRegressor
# 
# # Instantiate the model
# rf = RandomForestRegressor()
# 
# # Fit the model to the data
# rf.fit(X_train, y_train)
# 
# # print score of the model
# print_score(rf)
# 
# # visualizing the importance of features.
# visualize_importance(rf.feature_importances_, X_train)

"""## KNeighbors Regressor

KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood. The size of the neighbourhood needs to be set by the analyst or can be chosen using cross-validation to select the size that minimises the mean-squared error.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Fit a K-Neighbour Regressor model to the train dataset
# 
# # Import KNeighbourRegressor
# from sklearn.neighbors import KNeighborsRegressor
# 
# # Instantiate the model
# knn = KNeighborsRegressor()
# 
# # Fit the model to the data
# knn.fit(X_train, y_train)
# 
# # print score of the model
# print_score(knn)

"""## Gradient Boosting Regressor

Gradient Boosting Algorithm is generally used when we want to decrease the Bias error. it builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Fit a Gradient Boosting Regressor model to the train dataset
# 
# # Import GradientBoostingRegressor
# from sklearn.ensemble import GradientBoostingRegressor
# 
# # Instantiate the model
# gbr = GradientBoostingRegressor()
# 
# # Fit the model to the data
# gbr.fit(X_train, y_train)
# 
# # print score of the model
# print_score(gbr)
#             
# # visualizing the inportance of features.
# visualize_importance(gbr.feature_importances_, X_train)

"""## Decision Tree Regressor

Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Fit a Decision Tree Regressor model to the train dataset
# 
# # Import DecisionTreeRegressor
# from sklearn.tree import DecisionTreeRegressor
# 
# # Instantiate the model
# dtr = DecisionTreeRegressor()
# 
# # Fit the model to the data
# dtr.fit(X_train, y_train)
# 
# # print score of the model
# print_score(dtr)
# 
# # visualizing the inportance of features.
# visualize_importance(dtr.feature_importances_, X_train)

"""## AdaBoostRegressor

An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Fit a AdaBoost Regressor model to the train dataset
# 
# # Import AdaBoostRegressor
# from sklearn.ensemble import AdaBoostRegressor
# 
# # Instantiate the model
# Abr = AdaBoostRegressor()
# 
# # Fit the model to the data
# Abr.fit(X_train, y_train)
# 
# # print score of the model
# print_score(Abr)
# 
# # visualizing the inportance of features.
# visualize_importance(Abr.feature_importances_, X_train)

"""## XGB Regressor

XGBoost is an ensemble learning method. Sometimes, it may not be sufficient to rely upon the results of just one machine learning model. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Import XGBRegressor
# from xgboost import XGBRegressor
# 
# # Instantiate the model
# Xgb = XGBRegressor()
# 
# # Fit the model to the data
# Xgb.fit(X_train, y_train)
# 
# # print score of the model
# print_score(Xgb)
# 
# # visualizing the inportance of features.
# visualize_importance(Xgb.feature_importances_, X_train)

"""## Light Gradient Boosted Machine"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # import LGBMregressor
# from lightgbm import LGBMRegressor
# 
# # Instantiate the model
# Lgbm = LGBMRegressor()
# 
# # Fit the model to the data
# Lgbm.fit(X_train, y_train)
# 
# # print score of the model
# print_score(Lgbm)
# 
# # visualizing the inportance of features.
# visualize_importance(Lgbm.feature_importances_, X_train)

"""## Comparing all the model based on metric"""

# Helper function for comparing models metrics
def compare_models(models,names,X_train,y_train,X_test,y_test):
  # the libraries we need
  import sklearn.metrics as metrics
  from sklearn.model_selection import train_test_split


  # now, create a list with the objects 
  data = {'Metric':['rmse','MedAE','MAE','R-squared']}
  df_train = pd.DataFrame(data)
  df_test = pd.DataFrame(data)

  def rmse(x,y):
    return math.sqrt(((x-y)**2).mean())


  for (model,name) in zip(models,names):
    y_pred= model.predict(X_test) # then predict on the test set
    res = [rmse(model.predict(X_train), y_train),rmse(model.predict(X_test), y_test),
              metrics.median_absolute_error(model.predict(X_train), y_train),metrics.median_absolute_error(model.predict(X_test), y_test),
              metrics.mean_absolute_error(model.predict(X_train), y_train),metrics.mean_absolute_error(model.predict(X_test), y_test),
              metrics.r2_score(model.predict(X_train), y_train),metrics.r2_score(model.predict(X_test), y_test)]
    # get metrics of each model, and add to dataframe 
  df_train[name] = [res[0], res[2], res[4], res[6]]
  df_test[name] = [res[1], res[3], res[5], res[7]]
  return df_train,df_test



# list of models object
models = [LR, rf, knn, gbr, dtr, Abr, Xgb, Lgbm]

# list of models name
names = ['Linear Regression', 'Random Forest', 'Kneighbors', 'Gradient Boosting', 'Decision Tree','AdaBoost','XGBoost','LGBM']

# use function for comparing models by passing list of models object, names, train and test data
comp_model_train, comp_model_test = compare_models(models, names, X_train, y_train, X_test, y_test)

"""### RMSE of all models on train and test data"""

# printing rmse comparision of model on train and test
print(comp_model_train[:1])
print('\n')
print(comp_model_test[:1])

"""### All metrics on train and test data"""

# printing comparision of model on train and test
print("Results on Train data")
comp_model_train

print("Results on Test data")
comp_model_test

"""## Hyperparameter Tunning

A hyperparameter is a parameter whose value is set before the learning process begins.

Hyperparameters tuning is crucial as they control the overall behavior of a machine learning model.

Every machine learning models will have different hyperparameters that can be set.

## RamdomizedSearchCV

RandomizedSearchCV is very useful when we have many parameters to try and the training time is very long.

1. The first step is to write the parameters that we want to consider
2. From these parameters select the best ones.(which are printed in output)
"""

X_train,X_test,Y_train,Y_test=train_test_split(X[:5000],y[:5000],test_size=0.25,random_state=123)

X_train.shape, X_test.shape

# Use the random grid to search for best hyperparameters
def random_Search(model,X_train, Y_train,param_grid):
  from sklearn.model_selection import RandomizedSearchCV

  # Random search of parameters, using 3 fold cross validation, 
  # search across 100 different combinations, and use all available cores
  random = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model
  random.fit(X_train, Y_train)
  print("\n Best parameters: ",random.best_params_)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # create parameters dict in list for tunning
# rf_para_grid = {
#     'n_estimators': [int(x) for x in np.linspace(start=200, stop=400, num=3)],
#     'max_features': ['auto','sqrt'],
#     'max_depth' : [int(x) for x in np.linspace(10,110, num=3)],
#     'min_samples_split': [2,5],
#     'min_samples_leaf': [1,2],
#     'bootstrap': [True, False]
#     
#   
# }
# # passing data for hyper parameter tunning with RandomSearchCV
# random_Search(RandomForestRegressor(), X_train, Y_train, param_grid=rf_para_grid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # create parameters dict in list for tunning
# knn_para_grid = {
#     'leaf_size': list(range(1,20)),
#     'n_neighbors': list(range(1,30)),
#     'p':[1,2]
# }
# # passing data for hyper parameter tunning with RandomSearchCV
# random_Search(KNeighborsRegressor(), X_train, Y_train, param_grid=knn_para_grid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # create parameters dict in list for tunning
# Ada_para_grid = {
#     'n_estimators':[10,50,100],
#     'learning_rate':[0.0001,0.001,0.01]
# }
# # passing data for hyper parameter tunning with RandomSearchCV
# random_Search(AdaBoostRegressor(), X_train, Y_train, param_grid = Ada_para_grid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # create parameters dict in list for tunning
# XGB_para_grid = {
#     'learning_rate': [0.05,0.10,0.15],
#     'max_depth' : [3,4,5],
#     'min_child_weight':[1,3],
#     'gamma' : [0.0, 0.1],
#     'colsample_bytree': [0.3,0.4]
# }
# # passing data for hyper parameter tunning with RandomSearchCV
# random_Search(XGBRegressor(), X_train, Y_train, param_grid=XGB_para_grid)

# create parameters dict in list for tunning
GBR_para_grid = {
    'n_estimators': [10,50,100],
    'min_samples_split':[x for x in range(2,6)],
    'max_depth':[x for x in range(5,8)],
    'learning_rate':[0.1,0.2]
}
# passing data for hyper parameter tunning with RandomSearchCV
random_Search(GradientBoostingRegressor(), X_train, Y_train, param_grid=GBR_para_grid)

"""## Using best hyperparameters"""

X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.25,random_state=123)
X_train.shape,X_test.shape,Y_train.shape,Y_test.shape

"""## RandomForest Model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Instantiate the model with best parameters
# rf = RandomForestRegressor(**{'n_estimators':400, 'min_samples_split':5, 'max_features':'auto', 'max_depth':60,'bootstrap':True})
# 
# # Fit the model to the data
# rf.fit(X_train, Y_train)
# 
# # print score of the model
# print_score(rf)
# 
# # visualizing the importance of features.
# visualize_importance(rf.feature_importances_, X_train)

"""## KNN Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Instantiate the model with best parameters
# knn = KNeighborsRegressor(**{'p': 1, 'n_neighbors': 29, 'leaf_size': 19})
# 
# # Fit the model to the data
# knn.fit(X_train, Y_train)
# 
# # print score of the model
# print_score(knn)

"""## Adaboost Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Instantiate the model with best parameters
# Abr = AdaBoostRegressor(**{'n_estimators': 50, 'learning_rate': 0.001})
# 
# # Fit the model to the data
# Abr.fit(X_train, Y_train)
# 
# # print score of the model
# print_score(Abr)
# 
# # visualizing the inportance of features.
# visualize_importance(Abr.feature_importances_, X_train)

"""## XGBoost Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Instantiate the model with best parameters
# Xgb = XGBRegressor(** {'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.15, 'gamma': 0.0, 'colsample_bytree': 0.4})
# 
# # Fit the model to the data
# Xgb.fit(X_train, Y_train)
# 
# # print score of the model
# print_score(Xgb)
# 
# # visualizing the inportance of features.
# visualize_importance(Xgb.feature_importances_, X_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Initialize the models with best parameters
# GBR = GradientBoostingRegressor(**{'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.2})
# 
# # Fit the model to the data
# GBR.fit(X_train, Y_train)
# 
# # print score of the model
# print_score(GBR)
# 
# # visualizing the inportance of features.
# visualize_importance(GBR.feature_importances_, X_train)

# list of models object
models = [rf, knn, Abr, Xgb, GBR]
# list of models name
names = ['Random Forest', 'KNN', 'AdaBoost', 'XGBoost', 'GBR']
# use function for comparing models by passing list of models object, names, train and test data
comp_model_train, comp_model_test = compare_models(models, names, X_train, Y_train, X_test, Y_test)

# printing comparision of model on train and test
print("Results on Train data")
comp_model_train

print("Results on Test data")
comp_model_test

"""## CONCLUSION

In this project we have done an extensive Exploratory Data Analysis, visualized the data with amazing plots, build different models, visualized feature importance, did hyper parameter tunning of each model. Finally we found out that, from all the columns in the data Distance is the most important feature of them all and after Hyperparameter Tuning Adaboost Regressor performed much better when compared to remaining models.

"""